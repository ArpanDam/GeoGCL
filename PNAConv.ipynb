{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdsZ0jj9n6R7",
        "outputId": "618b45c9-c42b-4a98-ebf3-638c7a6a736f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.6.0+cu124\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.__version__)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQwUzA-Tf6Q-"
      },
      "source": [
        "## Required Libraries\n",
        "\n",
        "Before running this notebook, make sure you have installed the following libraries:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSuoZCH3oDhH",
        "outputId": "c7553f2b-4fe0-42f3-8197-2019a814d877"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pytorch-metric-learning\n",
            "  Downloading pytorch_metric_learning-2.8.1-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pytorch-metric-learning) (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from pytorch-metric-learning) (1.6.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-metric-learning) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from pytorch-metric-learning) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.6.0->pytorch-metric-learning)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.6.0->pytorch-metric-learning)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.6.0->pytorch-metric-learning)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.6.0->pytorch-metric-learning)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.6.0->pytorch-metric-learning)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.6.0->pytorch-metric-learning)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.6.0->pytorch-metric-learning)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.6.0->pytorch-metric-learning)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.6.0->pytorch-metric-learning)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (0.6.2)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch>=1.6.0->pytorch-metric-learning)\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.6.0->pytorch-metric-learning)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.6.0->pytorch-metric-learning) (1.3.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->pytorch-metric-learning) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->pytorch-metric-learning) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->pytorch-metric-learning) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.6.0->pytorch-metric-learning) (3.0.2)\n",
            "Downloading pytorch_metric_learning-2.8.1-py3-none-any.whl (125 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.9/125.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m115.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m91.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, pytorch-metric-learning\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 pytorch-metric-learning-2.8.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorch-metric-learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4TU0LtNdoDdj",
        "outputId": "bac51477-c879-4ca6-d9ba-d60239cca5e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.12.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2025.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.20.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2025.8.3)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.11/dist-packages (from aiosignal>=1.4.0->aiohttp->torch_geometric) (4.14.1)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.6.1\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.6.0+cu124.html\n",
            "Collecting pyg_lib\n",
            "  Downloading https://data.pyg.org/whl/torch-2.6.0%2Bcu124/pyg_lib-0.4.0%2Bpt26cu124-cp311-cp311-linux_x86_64.whl (4.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.6.0%2Bcu124/torch_scatter-2.1.2%2Bpt26cu124-cp311-cp311-linux_x86_64.whl (10.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m91.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.6.0%2Bcu124/torch_sparse-0.6.18%2Bpt26cu124-cp311-cp311-linux_x86_64.whl (5.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch_sparse) (1.16.1)\n",
            "Requirement already satisfied: numpy<2.6,>=1.25.2 in /usr/local/lib/python3.11/dist-packages (from scipy->torch_sparse) (2.0.2)\n",
            "Installing collected packages: torch_scatter, pyg_lib, torch_sparse\n",
            "Successfully installed pyg_lib-0.4.0+pt26cu124 torch_scatter-2.1.2+pt26cu124 torch_sparse-0.6.18+pt26cu124\n"
          ]
        }
      ],
      "source": [
        "!pip install torch_geometric\n",
        "!pip install pyg_lib torch_scatter torch_sparse -f https://data.pyg.org/whl/torch-$(python -c 'import torch; print(torch.__version__)').html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MIAnwWI8oDZ9"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import torch\n",
        "import random\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GCNConv\n",
        "import math\n",
        "from math import log\n",
        "\n",
        "file_path = 'Graph.pkl'\n",
        "\n",
        "\n",
        "eigen_vector='eigenvector.pkl'\n",
        "\n",
        "outdegree='outdegree.pkl'\n",
        "\n",
        "node_to_index_mapping='node_to_index_mapping.pkl'\n",
        "\n",
        "with open(file_path, 'rb') as file:\n",
        "    # Load the data from the file\n",
        "    edge_probability = pickle.load(file)\n",
        "\n",
        "with open(eigen_vector, 'rb') as file:\n",
        "    # Load the data from the file\n",
        "    eigenvector = pickle.load(file)\n",
        "\n",
        "\n",
        "with open(outdegree, 'rb') as file:\n",
        "    # Load the data from the file\n",
        "    outdegree = pickle.load(file)\n",
        "\n",
        "\n",
        "with open(node_to_index_mapping, 'rb') as file:\n",
        "    # Load the data from the file\n",
        "    node_to_index_mapping = pickle.load(file)\n",
        "\n",
        "def outdegree_graph1():\n",
        "    list_outdegree=[]\n",
        "    for key in outdegree:\n",
        "        list_outdegree.append(outdegree[key])\n",
        "    max_outdegree=max(list_outdegree)\n",
        "    min_outdegree=min(list_outdegree)\n",
        "\n",
        "    dict1_out_degree={}\n",
        "    list_follower=set()\n",
        "    for key in edge_probability:\n",
        "        dict1={}\n",
        "        for follower in edge_probability[key]:\n",
        "            list1=[]\n",
        "            for edge in edge_probability[key][follower]:\n",
        "                source_node_degree=outdegree[key]\n",
        "                tail_node_degree=outdegree[follower]\n",
        "                probability=float(source_node_degree+tail_node_degree)/2\n",
        "                # tag=edge[0]\n",
        "                list1.append((probability))\n",
        "            dict1[follower]=list1\n",
        "        dict1_out_degree[key]=dict1\n",
        "    return dict1_out_degree\n",
        "\n",
        "\n",
        "# augmenting with eigenvector\n",
        "\n",
        "def eigenvector1():\n",
        "    list_eigenvector=[]\n",
        "    for key in eigenvector:\n",
        "        list_eigenvector.append(eigenvector[key])\n",
        "\n",
        "    #indegree_average=sum(list_indegree)/len(list_indegree)\n",
        "    max_eigenvector=max(list_eigenvector)\n",
        "    min_eigenvector=min(list_eigenvector)\n",
        "    #print(\"\")\n",
        "\n",
        "    dict1_eigenvector={}  # this file will be like edge_probablity career having probaboility as outdegree\n",
        "\n",
        "\n",
        "    for key in edge_probability:\n",
        "        dict1={}\n",
        "        for follower in edge_probability[key]:\n",
        "            list1=[]\n",
        "            for edge in edge_probability[key][follower]:\n",
        "                source_node_degree=eigenvector[key]\n",
        "                tail_node_degree=eigenvector[follower]\n",
        "                probability=float(source_node_degree+tail_node_degree)/2\n",
        "                # tag=edge[0]\n",
        "                list1.append((probability))\n",
        "            dict1[follower]=list1\n",
        "        dict1_eigenvector[key]=dict1\n",
        "    return dict1_eigenvector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OX04hDmYhq0g"
      },
      "source": [
        "## Generates augmented graph data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DcKYO5KQoDWt"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import torch\n",
        "import random\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GCNConv\n",
        "import math\n",
        "from math import log\n",
        "import random\n",
        "\n",
        "file_path = 'Graph.pkl'\n",
        "\n",
        "\n",
        "eigen_vector='eigenvector.pkl'\n",
        "\n",
        "pagerank_file='page_rank.pkl'\n",
        "with open(pagerank_file, 'rb') as file:\n",
        "    pagerank = pickle.load(file)\n",
        "\n",
        "outdegree_file = 'outdegree.pkl'\n",
        "with open(outdegree_file, 'rb') as file:\n",
        "    outdegree = pickle.load(file)\n",
        "\n",
        "\n",
        "indegree_file = 'indegree.pkl'\n",
        "with open(indegree_file, 'rb') as file:\n",
        "    indegree = pickle.load(file)\n",
        "\n",
        "node_to_index_mapping='node_to_index_mapping.pkl'\n",
        "import numpy as np\n",
        "\n",
        "node_features=\"node_features.pkl\"\n",
        "\n",
        "#print(random_array)\n",
        "with open('Graph.pkl', 'rb') as file:\n",
        "    # Load the data from the file\n",
        "    edge_probability = pickle.load(file)\n",
        "\n",
        "\n",
        "with open('node_features.pkl', 'rb') as file:\n",
        "    # Load the data from the file\n",
        "    node_features = pickle.load(file)\n",
        "\n",
        "\n",
        "set_all_nodes = []\n",
        "\n",
        "# Add keys first\n",
        "for key in edge_probability:\n",
        "    if key not in set_all_nodes:\n",
        "        set_all_nodes.append(key)\n",
        "\n",
        "# Then add followers\n",
        "for key in edge_probability:\n",
        "    for follower in edge_probability[key]:\n",
        "        if follower not in set_all_nodes:\n",
        "            set_all_nodes.append(follower)\n",
        "\n",
        "\n",
        "dict_mapping={} # key node id value index id\n",
        "index=0\n",
        "\n",
        "for node in set_all_nodes:\n",
        "    dict_mapping[node]=index\n",
        "    index=index+1\n",
        "\n",
        "list_of_nodes = list(set_all_nodes)\n",
        "\n",
        "list_node_features=[] # 0 index node feature will be first\n",
        "\n",
        "for key in dict_mapping:\n",
        "    try:\n",
        "        list_node_features.append(node_features[key])\n",
        "    except:\n",
        "        # list_node_features.append(np.random.normal(loc=0, scale=1, size=100).astype(np.float32))\n",
        "        list_node_features.append(np.full((100,), 0.5, dtype=np.float32))\n",
        "\n",
        "# Load fairness score dictionary\n",
        "with open('node_fairness_scores.pkl', 'rb') as f:\n",
        "    fairness_score_dict = pickle.load(f)\n",
        "\n",
        "# Build fairness_of_nodes list aligned with dict_mapping order\n",
        "fairness_of_nodes = []\n",
        "for node in dict_mapping:\n",
        "    fairness_val = fairness_score_dict[node]  # fallback to random if missing\n",
        "    fairness_of_nodes.append(fairness_val)\n",
        "\n",
        "\n",
        "outdegree_graph=outdegree_graph1()\n",
        "\n",
        "\n",
        "eigenvector_graph=eigenvector1()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def func_graph_augmentation(random_augentation_number):\n",
        "#random_augentation_number=2\n",
        "\n",
        "    if(random_augentation_number==1):\n",
        "        dict_outdegree_edge_removed={}\n",
        "        for key in outdegree_graph:\n",
        "            dict1={}\n",
        "            for follower in outdegree_graph[key]:\n",
        "                list1=[]\n",
        "                for edge in outdegree_graph[key][follower]:\n",
        "                    if(edge*0.4>random.uniform(0, 1)):  # 0.5\n",
        "                    # if(edge[1]*0.4>.2):\n",
        "                        # tag=edge[0]\n",
        "                        probability=edge\n",
        "                        list1.append((probability))\n",
        "\n",
        "                \n",
        "                if(len(list1)>0):\n",
        "                    dict1[follower]=list1\n",
        "            if(follower in dict1):\n",
        "                dict_outdegree_edge_removed[key]=dict1\n",
        "        data_graph=graph_return(dict_outdegree_edge_removed)\n",
        "\n",
        "        return  data_graph\n",
        "\n",
        "\n",
        "    if(random_augentation_number==2):\n",
        "\n",
        "        dict_inf_probablity_removed={}\n",
        "        for key in edge_probability:\n",
        "            dict1={}\n",
        "            for follower in edge_probability[key]:\n",
        "                list1=[]\n",
        "                for edge in edge_probability[key][follower]:\n",
        "                    if(edge*0.8>random.uniform(0, 1)):   # 0.4\n",
        "                    # if(edge[1]*0.8>.2):\n",
        "                        # tag=edge[0]\n",
        "                        probability=edge\n",
        "                        list1.append((probability))\n",
        "                if(len(list1)>0):\n",
        "                    dict1[follower]=list1\n",
        "            if(follower in dict1):\n",
        "                dict_inf_probablity_removed[key]=dict1\n",
        "        data_graph=graph_return(dict_inf_probablity_removed)\n",
        "\n",
        "        return  data_graph\n",
        "# create the graph data here######################################################################################\n",
        "\n",
        "def graph_return(graph):\n",
        "    list_source=[]\n",
        "    list_target=[]\n",
        "    for key in graph:\n",
        "        for follower in graph[key]:\n",
        "            list_source.append(dict_mapping[key])\n",
        "            list_target.append(dict_mapping[follower])\n",
        "    edge_index = torch.tensor([list_source, list_target], dtype=torch.long)\n",
        "    node_features = torch.tensor(np.array(list_node_features), dtype=torch.float)\n",
        "    fairness_tensor = torch.tensor(fairness_of_nodes, dtype=torch.float64)  # Shape: [num_nodes]\n",
        "    data_graph = Data(\n",
        "        x=node_features,\n",
        "        edge_index=edge_index,\n",
        "        fairness=fairness_tensor,\n",
        "        node_id=torch.tensor(list(dict_mapping.keys()), dtype=torch.long)\n",
        "    )\n",
        "    return data_graph\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9wmpqrjgUKE"
      },
      "source": [
        "\n",
        "## **PNAConv-based GNN model** for 500 epochs and stores the learned node embeddings in `embeddings_fairness_mul.pkl`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OzkyZJfoDQT",
        "outputId": "892364e5-9eb6-4512-dd85-f5dc2706f5e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "svmem(total=8455864320, available=2739220480, percent=67.6, used=5716643840, free=2739220480)\n",
            "2.2.2+cu118\n",
            "2.6.1\n",
            "epoch 1, loss: 13.069145679473877\n",
            "epoch 2, loss: 9.985326337814332\n",
            "epoch 3, loss: 10.554227733612061\n",
            "epoch 4, loss: 9.407706451416015\n",
            "epoch 5, loss: 10.067283296585083\n",
            "epoch 6, loss: 8.841991424560547\n",
            "epoch 7, loss: 9.919242000579834\n",
            "epoch 8, loss: 9.011754989624023\n",
            "epoch 9, loss: 9.162083148956299\n",
            "epoch 10, loss: 8.731242227554322\n",
            "epoch 11, loss: 8.536951398849487\n",
            "epoch 12, loss: 8.862805032730103\n",
            "epoch 13, loss: 8.208715057373047\n",
            "epoch 14, loss: 8.389186143875122\n",
            "epoch 15, loss: 8.702409839630127\n",
            "epoch 16, loss: 7.996313190460205\n",
            "epoch 17, loss: 7.788460636138916\n",
            "epoch 18, loss: 7.940563821792603\n",
            "epoch 19, loss: 7.508432340621948\n",
            "epoch 20, loss: 7.920890235900879\n",
            "epoch 21, loss: 7.897067213058472\n",
            "epoch 22, loss: 7.178870820999146\n",
            "epoch 23, loss: 7.193622398376465\n",
            "epoch 24, loss: 7.015471267700195\n",
            "epoch 25, loss: 6.655825614929199\n",
            "epoch 26, loss: 8.203508043289185\n",
            "epoch 27, loss: 6.365209722518921\n",
            "epoch 28, loss: 6.229766893386841\n",
            "epoch 29, loss: 6.2465247631073\n",
            "epoch 30, loss: 6.6936431407928465\n",
            "epoch 31, loss: 6.638216733932495\n",
            "epoch 32, loss: 6.651311874389648\n",
            "epoch 33, loss: 6.339460492134094\n",
            "epoch 34, loss: 6.05044014453888\n",
            "epoch 35, loss: 5.936442494392395\n",
            "epoch 36, loss: 5.675639605522155\n",
            "epoch 37, loss: 6.488600826263427\n",
            "epoch 38, loss: 6.631077432632447\n",
            "epoch 39, loss: 5.788094544410706\n",
            "epoch 40, loss: 5.787393474578858\n",
            "epoch 41, loss: 5.456651782989502\n",
            "epoch 42, loss: 5.383569121360779\n",
            "epoch 43, loss: 5.248549151420593\n",
            "epoch 44, loss: 5.151453924179077\n",
            "epoch 45, loss: 5.046930432319641\n",
            "epoch 46, loss: 5.095026683807373\n",
            "epoch 47, loss: 5.097243976593018\n",
            "epoch 48, loss: 4.920618009567261\n",
            "epoch 49, loss: 4.817662072181702\n",
            "epoch 50, loss: 5.08777596950531\n",
            "epoch 51, loss: 5.14583203792572\n",
            "epoch 52, loss: 4.843447661399841\n",
            "epoch 53, loss: 4.603029894828796\n",
            "epoch 54, loss: 4.655335879325866\n",
            "epoch 55, loss: 4.512085652351379\n",
            "epoch 56, loss: 4.448870968818665\n",
            "epoch 57, loss: 4.3456711053848265\n",
            "epoch 58, loss: 4.380749225616455\n",
            "epoch 59, loss: 4.218315553665161\n",
            "epoch 60, loss: 4.388545298576355\n",
            "epoch 61, loss: 4.1909078598022464\n",
            "epoch 62, loss: 4.109708094596863\n",
            "epoch 63, loss: 4.109758186340332\n",
            "epoch 64, loss: 4.1849785327911375\n",
            "epoch 65, loss: 3.99908185005188\n",
            "epoch 66, loss: 3.9539166927337646\n",
            "epoch 67, loss: 3.9570844411849975\n",
            "epoch 68, loss: 3.902396035194397\n",
            "epoch 69, loss: 3.82376115322113\n",
            "epoch 70, loss: 3.774410772323608\n",
            "epoch 71, loss: 3.8774557590484617\n",
            "epoch 72, loss: 3.8128843307495117\n",
            "epoch 73, loss: 3.7693729639053344\n",
            "epoch 74, loss: 3.6866891145706178\n",
            "epoch 75, loss: 3.627634119987488\n",
            "epoch 76, loss: 3.6459448099136353\n",
            "epoch 77, loss: 3.643966054916382\n",
            "epoch 78, loss: 3.62938449382782\n",
            "epoch 79, loss: 3.575597143173218\n",
            "epoch 80, loss: 3.560252380371094\n",
            "epoch 81, loss: 3.5307278633117676\n",
            "epoch 82, loss: 3.50593159198761\n",
            "epoch 83, loss: 3.4749855279922484\n",
            "epoch 84, loss: 3.53390429019928\n",
            "epoch 85, loss: 3.542442870140076\n",
            "epoch 86, loss: 3.6029627323150635\n",
            "epoch 87, loss: 3.5787988662719727\n",
            "epoch 88, loss: 3.522111105918884\n",
            "epoch 89, loss: 3.5983244657516478\n",
            "epoch 90, loss: 3.492566633224487\n",
            "epoch 91, loss: 3.526030158996582\n",
            "epoch 92, loss: 3.576955485343933\n",
            "epoch 93, loss: 3.568907380104065\n",
            "epoch 94, loss: 3.515119433403015\n",
            "epoch 95, loss: 3.5466601848602295\n",
            "epoch 96, loss: 3.5772178888320925\n",
            "epoch 97, loss: 3.633743166923523\n",
            "epoch 98, loss: 3.5740440845489503\n",
            "epoch 99, loss: 3.5465868949890136\n",
            "epoch 100, loss: 3.5022057056427003\n",
            "epoch 101, loss: 3.5250255346298216\n",
            "epoch 102, loss: 3.5518394947052\n",
            "epoch 103, loss: 3.5704411029815675\n",
            "epoch 104, loss: 3.5046605110168456\n",
            "epoch 105, loss: 3.433224821090698\n",
            "epoch 106, loss: 3.395284128189087\n",
            "epoch 107, loss: 3.3590769529342652\n",
            "epoch 108, loss: 3.3567197799682615\n",
            "epoch 109, loss: 3.376016044616699\n",
            "epoch 110, loss: 3.3511916160583497\n",
            "epoch 111, loss: 3.3241339206695555\n",
            "epoch 112, loss: 3.3018125772476195\n",
            "epoch 113, loss: 3.2967422008514404\n",
            "epoch 114, loss: 3.3110671758651735\n",
            "epoch 115, loss: 3.2836854457855225\n",
            "epoch 116, loss: 3.269155168533325\n",
            "epoch 117, loss: 3.2493029594421388\n",
            "epoch 118, loss: 3.262013649940491\n",
            "epoch 119, loss: 3.241835188865662\n",
            "epoch 120, loss: 3.243103528022766\n",
            "epoch 121, loss: 3.2525650024414063\n",
            "epoch 122, loss: 3.2532876253128054\n",
            "epoch 123, loss: 3.2458359956741334\n",
            "epoch 124, loss: 3.222734236717224\n",
            "epoch 125, loss: 3.220544672012329\n",
            "epoch 126, loss: 3.2336528301239014\n",
            "epoch 127, loss: 3.250051498413086\n",
            "epoch 128, loss: 3.2337323665618896\n",
            "epoch 129, loss: 3.2289596796035767\n",
            "epoch 130, loss: 3.2151280641555786\n",
            "epoch 131, loss: 3.203115725517273\n",
            "epoch 132, loss: 3.2091737031936645\n",
            "epoch 133, loss: 3.203997564315796\n",
            "epoch 134, loss: 3.2309657096862794\n",
            "epoch 135, loss: 3.2085543155670164\n",
            "epoch 136, loss: 3.206939697265625\n",
            "epoch 137, loss: 3.1839024305343626\n",
            "epoch 138, loss: 3.188211131095886\n",
            "epoch 139, loss: 3.185530161857605\n",
            "epoch 140, loss: 3.1795583963394165\n",
            "epoch 141, loss: 3.1887142658233643\n",
            "epoch 142, loss: 3.1660303592681887\n",
            "epoch 143, loss: 3.16372230052948\n",
            "epoch 144, loss: 3.1513503551483155\n",
            "epoch 145, loss: 3.1303872585296633\n",
            "epoch 146, loss: 3.1268455266952513\n",
            "epoch 147, loss: 3.1212882518768312\n",
            "epoch 148, loss: 3.1178576946258545\n",
            "epoch 149, loss: 3.1232988834381104\n",
            "epoch 150, loss: 3.1234864234924316\n",
            "epoch 151, loss: 3.1242892503738404\n",
            "epoch 152, loss: 3.1279839515686034\n",
            "epoch 153, loss: 3.1167513608932493\n",
            "epoch 154, loss: 3.097440552711487\n",
            "epoch 155, loss: 3.1038745880126952\n",
            "epoch 156, loss: 3.0956842422485353\n",
            "epoch 157, loss: 3.1007586240768434\n",
            "epoch 158, loss: 3.112812042236328\n",
            "epoch 159, loss: 3.110335946083069\n",
            "epoch 160, loss: 3.1254042625427245\n",
            "epoch 161, loss: 3.1342691659927366\n",
            "epoch 162, loss: 3.131411409378052\n",
            "epoch 163, loss: 3.1292709827423097\n",
            "epoch 164, loss: 3.1423583745956423\n",
            "epoch 165, loss: 3.1236145734786986\n",
            "epoch 166, loss: 3.1332171678543093\n",
            "epoch 167, loss: 3.1253332138061523\n",
            "epoch 168, loss: 3.121330499649048\n",
            "epoch 169, loss: 3.111971950531006\n",
            "epoch 170, loss: 3.111060929298401\n",
            "epoch 171, loss: 3.1028362035751345\n",
            "epoch 172, loss: 3.1020300149917603\n",
            "epoch 173, loss: 3.1015912294387817\n",
            "epoch 174, loss: 3.0921070337295533\n",
            "epoch 175, loss: 3.083621692657471\n",
            "epoch 176, loss: 3.073473906517029\n",
            "epoch 177, loss: 3.0755399465560913\n",
            "epoch 178, loss: 3.076921057701111\n",
            "epoch 179, loss: 3.0720193862915037\n",
            "epoch 180, loss: 3.0918343544006346\n",
            "epoch 181, loss: 3.0864462852478027\n",
            "epoch 182, loss: 3.120283102989197\n",
            "epoch 183, loss: 3.109589695930481\n",
            "epoch 184, loss: 3.111360454559326\n",
            "epoch 185, loss: 3.095344829559326\n",
            "epoch 186, loss: 3.0852968454360963\n",
            "epoch 187, loss: 3.0633780717849732\n",
            "epoch 188, loss: 3.0665722608566286\n",
            "epoch 189, loss: 3.0768861055374144\n",
            "epoch 190, loss: 3.0636788845062255\n",
            "epoch 191, loss: 3.065063452720642\n",
            "epoch 192, loss: 3.068561553955078\n",
            "epoch 193, loss: 3.0970702171325684\n",
            "epoch 194, loss: 3.115853691101074\n",
            "epoch 195, loss: 3.1206982135772705\n",
            "epoch 196, loss: 3.123605799674988\n",
            "epoch 197, loss: 3.087488794326782\n",
            "epoch 198, loss: 3.0986857414245605\n",
            "epoch 199, loss: 3.0824939012527466\n",
            "epoch 200, loss: 3.0787577629089355\n",
            "epoch 201, loss: 3.0568288803100585\n",
            "epoch 202, loss: 3.0503784894943236\n",
            "epoch 203, loss: 3.053155016899109\n",
            "epoch 204, loss: 3.0436612606048583\n",
            "epoch 205, loss: 3.0507222175598145\n",
            "epoch 206, loss: 3.0446699380874636\n",
            "epoch 207, loss: 3.052012038230896\n",
            "epoch 208, loss: 3.057873821258545\n",
            "epoch 209, loss: 3.0674025058746337\n",
            "epoch 210, loss: 3.070151376724243\n",
            "epoch 211, loss: 3.0846129417419434\n",
            "epoch 212, loss: 3.0877734899520872\n",
            "epoch 213, loss: 3.079253363609314\n",
            "epoch 214, loss: 3.097295951843262\n",
            "epoch 215, loss: 3.103240704536438\n",
            "epoch 216, loss: 3.0979035615921022\n",
            "epoch 217, loss: 3.080512189865112\n",
            "epoch 218, loss: 3.054446744918823\n",
            "epoch 219, loss: 3.0663918972015383\n",
            "epoch 220, loss: 3.0714046239852903\n",
            "epoch 221, loss: 3.0550272703170775\n",
            "epoch 222, loss: 3.052834153175354\n",
            "epoch 223, loss: 3.0468674659729005\n",
            "epoch 224, loss: 3.0473011255264284\n",
            "epoch 225, loss: 3.0328488826751707\n",
            "epoch 226, loss: 3.0309931516647337\n",
            "epoch 227, loss: 3.0370235443115234\n",
            "epoch 228, loss: 3.0344993352890013\n",
            "epoch 229, loss: 3.0397501468658445\n",
            "epoch 230, loss: 3.0361335277557373\n",
            "epoch 231, loss: 3.0463565826416015\n",
            "epoch 232, loss: 3.0617422580718996\n",
            "epoch 233, loss: 3.054573106765747\n",
            "epoch 234, loss: 3.045620656013489\n",
            "epoch 235, loss: 3.0351932525634764\n",
            "epoch 236, loss: 3.036334276199341\n",
            "epoch 237, loss: 3.030533528327942\n",
            "epoch 238, loss: 3.0360231161117555\n",
            "epoch 239, loss: 3.0295055627822878\n",
            "epoch 240, loss: 3.0383163928985595\n",
            "epoch 241, loss: 3.034934568405151\n",
            "epoch 242, loss: 3.035732102394104\n",
            "epoch 243, loss: 3.037783908843994\n",
            "epoch 244, loss: 3.0305071115493774\n",
            "epoch 245, loss: 3.0270370721817015\n",
            "epoch 246, loss: 3.0274959802627563\n",
            "epoch 247, loss: 3.029155397415161\n",
            "epoch 248, loss: 3.0509850025177\n",
            "epoch 249, loss: 3.0618266344070433\n",
            "epoch 250, loss: 3.0472779750823973\n",
            "epoch 251, loss: 3.0523648262023926\n",
            "epoch 252, loss: 3.0578290700912474\n",
            "epoch 253, loss: 3.055873727798462\n",
            "epoch 254, loss: 3.074483370780945\n",
            "epoch 255, loss: 3.0740501403808596\n",
            "epoch 256, loss: 3.069088411331177\n",
            "epoch 257, loss: 3.0650389909744264\n",
            "epoch 258, loss: 3.062666821479797\n",
            "epoch 259, loss: 3.056083393096924\n",
            "epoch 260, loss: 3.0547909259796144\n",
            "epoch 261, loss: 3.043491554260254\n",
            "epoch 262, loss: 3.0254817724227907\n",
            "epoch 263, loss: 3.019708585739136\n",
            "epoch 264, loss: 3.018348526954651\n",
            "epoch 265, loss: 3.0158238410949707\n",
            "epoch 266, loss: 3.0086991786956787\n",
            "epoch 267, loss: 3.0198275566101076\n",
            "epoch 268, loss: 3.0184611082077026\n",
            "epoch 269, loss: 3.0235886573791504\n",
            "epoch 270, loss: 3.025008964538574\n",
            "epoch 271, loss: 3.038048839569092\n",
            "epoch 272, loss: 3.0483237504959106\n",
            "epoch 273, loss: 3.0429099082946776\n",
            "epoch 274, loss: 3.0640105247497558\n",
            "epoch 275, loss: 3.065060186386108\n",
            "epoch 276, loss: 3.0448862075805665\n",
            "epoch 277, loss: 3.066906380653381\n",
            "epoch 278, loss: 3.0524108171463014\n",
            "epoch 279, loss: 3.037837600708008\n",
            "epoch 280, loss: 3.032408666610718\n",
            "epoch 281, loss: 3.033998131752014\n",
            "epoch 282, loss: 3.03371376991272\n",
            "epoch 283, loss: 3.0358587741851806\n",
            "epoch 284, loss: 3.02809157371521\n",
            "epoch 285, loss: 3.0143614530563356\n",
            "epoch 286, loss: 2.999395990371704\n",
            "epoch 287, loss: 2.991663026809692\n",
            "epoch 288, loss: 2.996129560470581\n",
            "epoch 289, loss: 2.9885941982269286\n",
            "epoch 290, loss: 2.9843597173690797\n",
            "epoch 291, loss: 2.986814284324646\n",
            "epoch 292, loss: 2.9884811639785767\n",
            "epoch 293, loss: 2.983189058303833\n",
            "epoch 294, loss: 2.9822192192077637\n",
            "epoch 295, loss: 2.982744646072388\n",
            "epoch 296, loss: 2.980514407157898\n",
            "epoch 297, loss: 2.977368116378784\n",
            "epoch 298, loss: 2.9777466535568236\n",
            "epoch 299, loss: 2.974495530128479\n",
            "epoch 300, loss: 2.975794219970703\n",
            "epoch 301, loss: 2.97596390247345\n",
            "epoch 302, loss: 2.9765456438064577\n",
            "epoch 303, loss: 2.9769769668579102\n",
            "epoch 304, loss: 2.9776009559631347\n",
            "epoch 305, loss: 2.9850775957107545\n",
            "epoch 306, loss: 2.9789800882339477\n",
            "epoch 307, loss: 2.985697627067566\n",
            "epoch 308, loss: 2.987591600418091\n",
            "epoch 309, loss: 2.9822709560394287\n",
            "epoch 310, loss: 2.9778150796890257\n",
            "epoch 311, loss: 2.9743559837341307\n",
            "epoch 312, loss: 2.975404381752014\n",
            "epoch 313, loss: 2.9736428260803223\n",
            "epoch 314, loss: 2.974744462966919\n",
            "epoch 315, loss: 2.97601854801178\n",
            "epoch 316, loss: 2.97797749042511\n",
            "epoch 317, loss: 2.9829707384109496\n",
            "epoch 318, loss: 2.9828469276428224\n",
            "epoch 319, loss: 2.98909387588501\n",
            "epoch 320, loss: 2.9785687923431396\n",
            "epoch 321, loss: 2.9830627918243406\n",
            "epoch 322, loss: 2.974272084236145\n",
            "epoch 323, loss: 2.9747905015945433\n",
            "epoch 324, loss: 2.9672124862670897\n",
            "epoch 325, loss: 2.976923179626465\n",
            "epoch 326, loss: 2.9693440437316894\n",
            "epoch 327, loss: 2.9686863899230955\n",
            "epoch 328, loss: 2.9671037435531615\n",
            "epoch 329, loss: 2.971509575843811\n",
            "epoch 330, loss: 2.9690847873687742\n",
            "epoch 331, loss: 2.9732661485671996\n",
            "epoch 332, loss: 2.9669657945632935\n",
            "epoch 333, loss: 2.967185711860657\n",
            "epoch 334, loss: 2.9655198574066164\n",
            "epoch 335, loss: 2.9696311712265016\n",
            "epoch 336, loss: 2.965602445602417\n",
            "epoch 337, loss: 2.9664241790771486\n",
            "epoch 338, loss: 2.970662307739258\n",
            "epoch 339, loss: 2.9655341625213625\n",
            "epoch 340, loss: 2.963062047958374\n",
            "epoch 341, loss: 2.961425232887268\n",
            "epoch 342, loss: 2.9650949239730835\n",
            "epoch 343, loss: 2.9649470329284666\n",
            "epoch 344, loss: 2.9601794719696044\n",
            "epoch 345, loss: 2.9593230724334716\n",
            "epoch 346, loss: 2.965531277656555\n",
            "epoch 347, loss: 2.9595436096191405\n",
            "epoch 348, loss: 2.958732271194458\n",
            "epoch 349, loss: 2.9586273193359376\n",
            "epoch 350, loss: 2.9593091011047363\n",
            "epoch 351, loss: 2.9565168142318727\n",
            "epoch 352, loss: 2.956363296508789\n",
            "epoch 353, loss: 2.960872530937195\n",
            "epoch 354, loss: 2.967607045173645\n",
            "epoch 355, loss: 2.968136119842529\n",
            "epoch 356, loss: 2.9650676012039185\n",
            "epoch 357, loss: 2.96349835395813\n",
            "epoch 358, loss: 2.967417669296265\n",
            "epoch 359, loss: 2.9848886489868165\n",
            "epoch 360, loss: 2.993835616111755\n",
            "epoch 361, loss: 2.994470238685608\n",
            "epoch 362, loss: 2.984652519226074\n",
            "epoch 363, loss: 2.979846477508545\n",
            "epoch 364, loss: 2.994537854194641\n",
            "epoch 365, loss: 2.9994889736175536\n",
            "epoch 366, loss: 2.988828492164612\n",
            "epoch 367, loss: 2.9931254625320434\n",
            "epoch 368, loss: 2.9852957725524902\n",
            "epoch 369, loss: 2.982466435432434\n",
            "epoch 370, loss: 2.9887507915496827\n",
            "epoch 371, loss: 2.9772468328475954\n",
            "epoch 372, loss: 2.972988510131836\n",
            "epoch 373, loss: 2.978104305267334\n",
            "epoch 374, loss: 2.9726956605911257\n",
            "epoch 375, loss: 2.9676525354385377\n",
            "epoch 376, loss: 2.9630409240722657\n",
            "epoch 377, loss: 2.960808253288269\n",
            "epoch 378, loss: 2.9565718173980713\n",
            "epoch 379, loss: 2.9591546773910524\n",
            "epoch 380, loss: 2.956977891921997\n",
            "epoch 381, loss: 2.9551605701446535\n",
            "epoch 382, loss: 2.9560826301574705\n",
            "epoch 383, loss: 2.9635777473449707\n",
            "epoch 384, loss: 2.9612293004989625\n",
            "epoch 385, loss: 2.956008791923523\n",
            "epoch 386, loss: 2.9572110414505004\n",
            "epoch 387, loss: 2.960168886184692\n",
            "epoch 388, loss: 2.9561671018600464\n",
            "epoch 389, loss: 2.952292466163635\n",
            "epoch 390, loss: 2.949116611480713\n",
            "epoch 391, loss: 2.9477170944213866\n",
            "epoch 392, loss: 2.9456790685653687\n",
            "epoch 393, loss: 2.9458144187927244\n",
            "epoch 394, loss: 2.9426594495773317\n",
            "epoch 395, loss: 2.9447638034820556\n",
            "epoch 396, loss: 2.9436797142028808\n",
            "epoch 397, loss: 2.943372893333435\n",
            "epoch 398, loss: 2.9412060499191286\n",
            "epoch 399, loss: 2.9423472404479982\n",
            "epoch 400, loss: 2.941800928115845\n",
            "epoch 401, loss: 2.9450302362442016\n",
            "epoch 402, loss: 2.946296215057373\n",
            "epoch 403, loss: 2.9501603841781616\n",
            "epoch 404, loss: 2.9502546787261963\n",
            "epoch 405, loss: 2.948888635635376\n",
            "epoch 406, loss: 2.9453468799591063\n",
            "epoch 407, loss: 2.9444928884506227\n",
            "epoch 408, loss: 2.94267418384552\n",
            "epoch 409, loss: 2.9449705839157105\n",
            "epoch 410, loss: 2.944952940940857\n",
            "epoch 411, loss: 2.9467597007751465\n",
            "epoch 412, loss: 2.9464701414108276\n",
            "epoch 413, loss: 2.9510459184646605\n",
            "epoch 414, loss: 2.9519339323043825\n",
            "epoch 415, loss: 2.9607295274734495\n",
            "epoch 416, loss: 2.9702242612838745\n",
            "epoch 417, loss: 2.975460147857666\n",
            "epoch 418, loss: 2.9837436437606812\n",
            "epoch 419, loss: 2.9733461141586304\n",
            "epoch 420, loss: 2.9723142862319945\n",
            "epoch 421, loss: 2.9655294895172117\n",
            "epoch 422, loss: 2.956649732589722\n",
            "epoch 423, loss: 2.9562246084213255\n",
            "epoch 424, loss: 2.9571391582489013\n",
            "epoch 425, loss: 2.9486920833587646\n",
            "epoch 426, loss: 2.9558573722839356\n",
            "epoch 427, loss: 2.959973454475403\n",
            "epoch 428, loss: 2.9545878171920776\n",
            "epoch 429, loss: 2.9494481563568113\n",
            "epoch 430, loss: 2.9442087173461915\n",
            "epoch 431, loss: 2.9385166645050047\n",
            "epoch 432, loss: 2.942187547683716\n",
            "epoch 433, loss: 2.9415920257568358\n",
            "epoch 434, loss: 2.9419010400772097\n",
            "epoch 435, loss: 2.942220616340637\n",
            "epoch 436, loss: 2.9399553775787353\n",
            "epoch 437, loss: 2.943777656555176\n",
            "epoch 438, loss: 2.945305347442627\n",
            "epoch 439, loss: 2.940966248512268\n",
            "epoch 440, loss: 2.9469225883483885\n",
            "epoch 441, loss: 2.9424007654190065\n",
            "epoch 442, loss: 2.941311240196228\n",
            "epoch 443, loss: 2.944378447532654\n",
            "epoch 444, loss: 2.9395655155181886\n",
            "epoch 445, loss: 2.9432292222976684\n",
            "epoch 446, loss: 2.942477321624756\n",
            "epoch 447, loss: 2.9375101566314696\n",
            "epoch 448, loss: 2.9355864763259887\n",
            "epoch 449, loss: 2.935131287574768\n",
            "epoch 450, loss: 2.9343578815460205\n",
            "epoch 451, loss: 2.9353100299835204\n",
            "epoch 452, loss: 2.934106707572937\n",
            "epoch 453, loss: 2.9347707986831666\n",
            "epoch 454, loss: 2.9351767778396605\n",
            "epoch 455, loss: 2.9370362997055053\n",
            "epoch 456, loss: 2.9356470108032227\n",
            "epoch 457, loss: 2.942988872528076\n",
            "epoch 458, loss: 2.943291115760803\n",
            "epoch 459, loss: 2.938019323348999\n",
            "epoch 460, loss: 2.9371416091918947\n",
            "epoch 461, loss: 2.940880560874939\n",
            "epoch 462, loss: 2.943197774887085\n",
            "epoch 463, loss: 2.9452513217926026\n",
            "epoch 464, loss: 2.9363044023513796\n",
            "epoch 465, loss: 2.936014175415039\n",
            "epoch 466, loss: 2.935570311546326\n",
            "epoch 467, loss: 2.9326232433319093\n",
            "epoch 468, loss: 2.930622601509094\n",
            "epoch 469, loss: 2.93183696269989\n",
            "epoch 470, loss: 2.9294816493988036\n",
            "epoch 471, loss: 2.9305320501327516\n",
            "epoch 472, loss: 2.932384204864502\n",
            "epoch 473, loss: 2.9330104112625124\n",
            "epoch 474, loss: 2.932637929916382\n",
            "epoch 475, loss: 2.9363217830657957\n",
            "epoch 476, loss: 2.9325379610061644\n",
            "epoch 477, loss: 2.934618663787842\n",
            "epoch 478, loss: 2.933035063743591\n",
            "epoch 479, loss: 2.9341324090957643\n",
            "epoch 480, loss: 2.933902382850647\n",
            "epoch 481, loss: 2.946037936210632\n",
            "epoch 482, loss: 2.958061408996582\n",
            "epoch 483, loss: 2.9765574455261232\n",
            "epoch 484, loss: 2.960057592391968\n",
            "epoch 485, loss: 2.970167064666748\n",
            "epoch 486, loss: 2.96519935131073\n",
            "epoch 487, loss: 2.966853642463684\n",
            "epoch 488, loss: 2.9584959506988526\n",
            "epoch 489, loss: 2.9616862297058106\n",
            "epoch 490, loss: 2.9480735778808596\n",
            "epoch 491, loss: 2.9463133811950684\n",
            "epoch 492, loss: 2.938133716583252\n",
            "epoch 493, loss: 2.9395689964294434\n",
            "epoch 494, loss: 2.9391380071640016\n",
            "epoch 495, loss: 2.93921103477478\n",
            "epoch 496, loss: 2.9378344774246217\n",
            "epoch 497, loss: 2.948566722869873\n",
            "epoch 498, loss: 2.9447866916656493\n",
            "epoch 499, loss: 2.943758010864258\n",
            "epoch 500, loss: 2.9401612758636473\n",
            "Saved 1095 node embeddings for Graph1 to 'embeddings_graph1.pkl'\n",
            "Saved 1095 node embeddings for Graph2 to 'embeddings_graph2.pkl'\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import pickle\n",
        "import torch\n",
        "import random\n",
        "from torch_geometric.data import Data\n",
        "import torch.optim as optim\n",
        "#from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.loader import ClusterData, ClusterLoader\n",
        "from torch_geometric.loader import NeighborLoader\n",
        "from torch_geometric.nn import PNAConv\n",
        "#import input_graph_creation\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "#import input_small_graph_creation\n",
        "from pytorch_metric_learning.losses import NTXentLoss\n",
        "import torch_geometric\n",
        "from torch_geometric.loader import DataLoader\n",
        "import gc\n",
        "import psutil\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.utils import k_hop_subgraph\n",
        "# Print memory usage\n",
        "print(psutil.virtual_memory())\n",
        "print(torch.__version__)\n",
        "print(torch_geometric.__version__)\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import PNAConv\n",
        "\n",
        "\n",
        "\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, perceptron_hidden_dim, degree):\n",
        "        super().__init__()\n",
        "        self.conv1 = PNAConv(\n",
        "            in_channels, hidden_channels,\n",
        "            [\"sum\",\"mean\",\"min\",\"max\",\"var\",\"std\"],\n",
        "            [\"identity\",\"amplification\",\"attenuation\",\"linear\",\"inverse_linear\"],\n",
        "            degree\n",
        "        )\n",
        "        self.conv2 = PNAConv(\n",
        "            hidden_channels, out_channels,\n",
        "            [\"sum\",\"mean\",\"min\",\"max\",\"var\",\"std\"],\n",
        "            [\"identity\",\"amplification\",\"attenuation\",\"linear\",\"inverse_linear\"],\n",
        "            degree\n",
        "        )\n",
        "        self.fc1 = torch.nn.Linear(out_channels, perceptron_hidden_dim)\n",
        "        self.fc2 = torch.nn.Linear(perceptron_hidden_dim, out_channels)\n",
        "\n",
        "    def forward(self, data1st_graph, data2nd_graph):\n",
        "        # ---- First graph ----\n",
        "        x, edge_index = data1st_graph.x, data1st_graph.edge_index\n",
        "        x = self.conv1(x, edge_index)     # no edge_attr\n",
        "        x = torch.relu(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x_graph1 = self.fc2(x)\n",
        "\n",
        "        # ---- Second graph ----\n",
        "        x, edge_index = data2nd_graph.x, data2nd_graph.edge_index\n",
        "        x = self.conv1(x, edge_index)     # no edge_attr\n",
        "        x = torch.relu(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x_graph2 = self.fc2(x)\n",
        "\n",
        "        return x_graph1, x_graph2\n",
        "# loss_func = NTXentLoss(temperature=0.10)\n",
        "\n",
        "with open(\"Graph.pkl\", \"rb\") as f:\n",
        "    edge_probability = pickle.load(f)\n",
        "\n",
        "\n",
        "def fairness_contrastive_loss(h1, h2, node_ids1, node_ids2,\n",
        "                              combined_hops, fairness_scores,\n",
        "                              alpha_max=0.8, eta=1.0,\n",
        "                              negative_sample_size=256,\n",
        "                              temperature=0.1):\n",
        "    \"\"\"\n",
        "    Fairness-aware contrastive loss using embeddings from two augmented graphs (h1, h2).\n",
        "    h1: embeddings from G1\n",
        "    h2: embeddings from G2\n",
        "    node_ids1: node ids corresponding to h1\n",
        "    node_ids2: node ids corresponding to h2\n",
        "    \"\"\"\n",
        "    device = h1.device\n",
        "    node_id_to_index_1 = {int(nid): idx for idx, nid in enumerate(node_ids1.tolist())}\n",
        "    node_id_to_index_2 = {int(nid): idx for idx, nid in enumerate(node_ids2.tolist())}\n",
        "\n",
        "    total_loss = torch.tensor(0.0, device=device)\n",
        "    influencer_count = 0\n",
        "\n",
        "    for u, two_hop_followers in combined_hops.items():\n",
        "        if u not in fairness_scores or u not in node_id_to_index_1:\n",
        "            continue\n",
        "\n",
        "        u_index = node_id_to_index_1[u]\n",
        "        u_embed = h1[u_index]   # h^G1_u\n",
        "        F_u = torch.tensor(fairness_scores[u], dtype=torch.float, device=device)\n",
        "\n",
        "        # ---------- Positive term ----------\n",
        "        # L⁺_u = - (1 / |NG2(u)|) Σ_{v∈NG2(u)} F(u) · w_uv · log σ(sim(h^G1_u, h^G2_v) / τ)\n",
        "        L_pos = torch.tensor(0.0, device=device)\n",
        "        pos_count = 0\n",
        "        S_u = torch.tensor(0.0, device=device)  # structural dominance accumulator\n",
        "\n",
        "        for v in two_hop_followers:\n",
        "            if v not in node_id_to_index_2:\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                w_uv = edge_probability[u][v][0]  # AIS-based edge weight\n",
        "            except KeyError:\n",
        "                w_uv = 1.0\n",
        "\n",
        "            v_index = node_id_to_index_2[v]\n",
        "            sim_uv = F.cosine_similarity(u_embed.unsqueeze(0), h2[v_index].unsqueeze(0)).squeeze()\n",
        "\n",
        "            L_pos += - w_uv * F_u * torch.log(torch.sigmoid(sim_uv / temperature) + 1e-8)\n",
        "            S_u += torch.clamp(sim_uv, min=0.0)  # accumulate structural dominance\n",
        "            pos_count += 1\n",
        "\n",
        "        if pos_count > 0:\n",
        "            L_pos /= pos_count\n",
        "\n",
        "        # ---------- Negative term ----------\n",
        "        # L⁻_u = - (1 / Z⁻_u) Σ_{n ∈ V \\ Fu} log(1 - σ(sim(h^G1_u, h^G2_n) / τ))\n",
        "        all_node_ids_2 = set(node_id_to_index_2.keys())\n",
        "        neg_candidates = list(all_node_ids_2 - set(two_hop_followers) - {u})\n",
        "        sampled_negatives = random.sample(neg_candidates, min(negative_sample_size, len(neg_candidates)))\n",
        "\n",
        "        L_neg = torch.tensor(0.0, device=device)\n",
        "        for n in sampled_negatives:\n",
        "            n_index = node_id_to_index_2[n]\n",
        "            sim_un = F.cosine_similarity(u_embed.unsqueeze(0), h2[n_index].unsqueeze(0)).squeeze()\n",
        "            L_neg += - torch.log(1 - torch.sigmoid(sim_un / temperature) + 1e-8)\n",
        "\n",
        "        if len(sampled_negatives) > 0:\n",
        "            L_neg /= len(sampled_negatives)\n",
        "\n",
        "        # ---------- Structural dominance ----------\n",
        "        # S(u) = Σ_{v∈N₂-hop(u)} max(0, sim(h_u, h_v))\n",
        "        # D(u) = min–max normalize(S(u)) ∈ [0,1]   (here: clamped version)\n",
        "        D_u = torch.clamp(S_u, 0.0, 1.0)\n",
        "\n",
        "        # ---------- Adaptive multiplier ----------\n",
        "        # α(u) = clip(η · D(u) · (1 - F(u)), 0, α_max)\n",
        "        alpha_u = torch.clamp(eta * D_u * (1 - F_u), 0.0, alpha_max)\n",
        "\n",
        "        # ---------- Adaptive fairness loss ----------\n",
        "        # L_fair-adapt(u) = L⁺_u + (1 + α(u)) · L⁻_u\n",
        "        L_u = L_pos + (1 + alpha_u) * L_neg\n",
        "        total_loss += L_u\n",
        "        influencer_count += 1\n",
        "\n",
        "    # ---------- Final objective ----------\n",
        "    # L_total = (1 / |U|) Σ_{u∈U} L(u)\n",
        "    if influencer_count > 0:\n",
        "        return total_loss / influencer_count\n",
        "    else:\n",
        "        return torch.tensor(0.0, device=device)\n",
        "\n",
        "\n",
        "\n",
        "def nt_xent_loss(z1, z2, temperature=0.1):\n",
        "    # N = batch size (take min of z1 and z2 in case they differ)\n",
        "    N = min(z1.size(0), z2.size(0))\n",
        "    z1, z2 = z1[:N], z2[:N]\n",
        "    # f = fairness_weights[:N]  # (commented out for now)\n",
        "\n",
        "    # Concatenate embeddings from both views → shape [2N, D]\n",
        "    z = torch.cat([z1, z2], dim=0)  # z = [z1; z2]\n",
        "\n",
        "    # Normalize embeddings so cosine similarity = dot product\n",
        "    z = F.normalize(z, dim=1)\n",
        "\n",
        "    # Compute similarity matrix: sim_matrix[i, j] = (z[i] · z[j]) / τ\n",
        "    sim_matrix = torch.mm(z, z.t()) / temperature  # shape [2N, 2N]\n",
        "\n",
        "    # Mask for removing self-similarity (diagonal entries)\n",
        "    mask = torch.eye(2 * N, dtype=torch.bool, device=z.device)\n",
        "\n",
        "    loss = 0.0\n",
        "    for idx in range(N):\n",
        "        # Positive pair similarity: s(z1[i], z2[i]) = exp( (z1[i]·z2[i]) / τ )\n",
        "        pos_sim = torch.exp(F.cosine_similarity(\n",
        "            z1[idx].unsqueeze(0), \n",
        "            z2[idx].unsqueeze(0)\n",
        "        ) / temperature)\n",
        "\n",
        "        # Similarity of z1[i] with all others (exp scaled by τ)\n",
        "        all_sims_1 = torch.exp(sim_matrix[idx, :])\n",
        "\n",
        "        # Similarity of z2[i] with all others\n",
        "        all_sims_2 = torch.exp(sim_matrix[idx + N, :])\n",
        "\n",
        "        # Combine both views’ similarities\n",
        "        all_sims = all_sims_1 + all_sims_2\n",
        "\n",
        "        # Remove self-similarity (don’t compare z1[i] with itself, etc.)\n",
        "        all_sims = all_sims[~mask[idx]]\n",
        "\n",
        "        # Denominator: sum over all possible pairs (negatives + positive)\n",
        "        denom = all_sims.sum() + 1e-8\n",
        "\n",
        "        # NT-Xent loss for this node:\n",
        "        # L_i = -log( exp(sim_pos/τ) / Σ_j exp(sim(z_i, z_j)/τ) )\n",
        "        node_loss = -torch.log(pos_sim / denom)\n",
        "\n",
        "        # Apply weight (if fairness_weights used later, replace 1 with f[i])\n",
        "        weight = 1\n",
        "        loss += (weight) * node_loss\n",
        "\n",
        "    # Final loss = mean over batch\n",
        "    return loss / N\n",
        "\n",
        "\n",
        "\n",
        "def train(train_loader, train_loader2, combined_hops, multi_hop, fairness_scores, λ=1.0,\n",
        "          alpha_max=5, eta=1.0, negative_sample_size=256, temperature=0.1):\n",
        "    \"\"\"\n",
        "    Training loop using NT-Xent loss + fairness-aware contrastive loss.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    total_loss_list = []\n",
        "\n",
        "    for data1st_graph, data2nd_graph in zip(train_loader, train_loader2):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Compute embeddings from the model\n",
        "        h_1, h_2 = model(data1st_graph, data2nd_graph)\n",
        "\n",
        "        # Match sizes if needed\n",
        "        if h_2.shape[0] > h_1.shape[0]:\n",
        "            h_2 = h_2[:h_1.shape[0]]\n",
        "        else:\n",
        "            h_1 = h_1[:h_2.shape[0]]\n",
        "\n",
        "\n",
        "        # ---------- Contrastive-Loss ----------\n",
        "        loss_ntx = nt_xent_loss(h_1, h_2, temperature=temperature)\n",
        "\n",
        "\n",
        "        # ---------- Fairness-Aware Contrastive Loss ----------\n",
        "\n",
        "        loss_fair = fairness_contrastive_loss(\n",
        "                h_1, h_2,\n",
        "                node_ids1=data1st_graph.node_id[:h_1.size(0)],\n",
        "                node_ids2=data2nd_graph.node_id[:h_2.size(0)],\n",
        "                combined_hops=combined_hops,\n",
        "                fairness_scores=fairness_scores,\n",
        "                alpha_max=alpha_max,\n",
        "                eta=eta,\n",
        "                negative_sample_size=negative_sample_size,\n",
        "                temperature=temperature\n",
        "            )\n",
        "\n",
        "\n",
        "        # ---------- Total Loss ----------\n",
        "        total_loss = loss_ntx + λ * loss_fair\n",
        "\n",
        "        # Backpropagation\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss_list.append(total_loss.item())\n",
        "\n",
        "    return sum(total_loss_list) / len(total_loss_list)\n",
        "\n",
        "\n",
        "\n",
        "def save_embeddings(model, loader1, loader2, output_path_g1=\"embeddings_graph1.pkl\", output_path_g2=\"embeddings_graph2.pkl\"):\n",
        "    model.eval()\n",
        "    all_embeddings_g1 = []\n",
        "    all_embeddings_g2 = []\n",
        "    all_node_ids_g1 = []\n",
        "    all_node_ids_g2 = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data1, data2 in zip(loader1, loader2):\n",
        "            # Move to device\n",
        "            data1 = data1.to(next(model.parameters()).device)\n",
        "            data2 = data2.to(next(model.parameters()).device)\n",
        "\n",
        "            # Get embeddings separately\n",
        "            h1, h2 = model(data1, data2)\n",
        "\n",
        "            # Truncate to align with node_ids\n",
        "            min_len1 = min(h1.size(0), data1.node_id.size(0))\n",
        "            min_len2 = min(h2.size(0), data2.node_id.size(0))\n",
        "\n",
        "            h1 = h1[:min_len1]\n",
        "            h2 = h2[:min_len2]\n",
        "\n",
        "            node_ids1 = data1.node_id[:min_len1]\n",
        "            node_ids2 = data2.node_id[:min_len2]\n",
        "\n",
        "            all_embeddings_g1.append(h1.cpu())\n",
        "            all_embeddings_g2.append(h2.cpu())\n",
        "            all_node_ids_g1.extend(node_ids1.cpu().numpy())\n",
        "            all_node_ids_g2.extend(node_ids2.cpu().numpy())\n",
        "\n",
        "    # Convert to numpy\n",
        "    final_embeddings_g1 = torch.cat(all_embeddings_g1, dim=0).numpy()\n",
        "    final_embeddings_g2 = torch.cat(all_embeddings_g2, dim=0).numpy()\n",
        "\n",
        "    # Save Graph1 embeddings\n",
        "    embeddings_dict_g1 = {\n",
        "        \"node_ids\": all_node_ids_g1,\n",
        "        \"embeddings\": final_embeddings_g1,\n",
        "    }\n",
        "    with open(output_path_g1, \"wb\") as f:\n",
        "        pickle.dump(embeddings_dict_g1, f)\n",
        "    print(f\"Saved {len(all_node_ids_g1)} node embeddings for Graph1 to '{output_path_g1}'\")\n",
        "\n",
        "    # Save Graph2 embeddings\n",
        "    embeddings_dict_g2 = {\n",
        "        \"node_ids\": all_node_ids_g2,\n",
        "        \"embeddings\": final_embeddings_g2,\n",
        "    }\n",
        "    with open(output_path_g2, \"wb\") as f:\n",
        "        pickle.dump(embeddings_dict_g2, f)\n",
        "    print(f\"Saved {len(all_node_ids_g2)} node embeddings for Graph2 to '{output_path_g2}'\")\n",
        "\n",
        "\n",
        "def manual_cluster_split(input_graph1, input_graph2, num_parts=10):\n",
        "    assert input_graph1.num_nodes == input_graph2.num_nodes, \"Node count mismatch!\"\n",
        "    num_nodes = input_graph1.num_nodes\n",
        "    # node_indices = torch.randperm(num_nodes)\n",
        "    node_indices = torch.arange(num_nodes)\n",
        "    clusters_1 = []\n",
        "    clusters_2 = []\n",
        "\n",
        "    part_size = (num_nodes + num_parts - 1) // num_parts  # ceiling division\n",
        "    for i in range(0, num_nodes, part_size):\n",
        "      idx = node_indices[i:i+part_size]\n",
        "\n",
        "      sub1 = input_graph1.subgraph(idx)\n",
        "      sub2 = input_graph2.subgraph(idx)\n",
        "\n",
        "      sub1.node_id = input_graph1.node_id[idx]\n",
        "      sub2.node_id = input_graph2.node_id[idx]\n",
        "\n",
        "      sub1.fairness = input_graph1.fairness[idx]\n",
        "      sub2.fairness = input_graph2.fairness[idx]\n",
        "\n",
        "      clusters_1.append(sub1)\n",
        "      clusters_2.append(sub2)\n",
        "\n",
        "    return clusters_1, clusters_2\n",
        "\n",
        "def loader_func(input_graph1, input_graph2):\n",
        "    # Get matching clusters from both graphs\n",
        "    cluster_subgraphs_1, cluster_subgraphs_2 = manual_cluster_split(input_graph1, input_graph2, num_parts=10)\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader_1 = DataLoader(cluster_subgraphs_1, batch_size=1, shuffle=False)\n",
        "    train_loader_2 = DataLoader(cluster_subgraphs_2, batch_size=1, shuffle=False)\n",
        "\n",
        "    return train_loader_1, train_loader_2\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "in_channels=100\n",
        "hidden_channels = 60  # Example value, you can change it\n",
        "#out_channels = 120  # Example value, you can change it\n",
        "out_channels = 50\n",
        "#perceptron_hidden_dim = 250\n",
        "perceptron_hidden_dim = 60\n",
        "degree=torch.tensor([240, 328,  79,  39,  23,  12,  11,   7,   6,   5,   7,   3,   1,   0,\n",
        "          2,   0,   0,   0,   1])\n",
        "\n",
        "model = Net(in_channels, hidden_channels, out_channels,perceptron_hidden_dim,degree)\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-5)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "with open(\"multi_hop_followers.pkl\", \"rb\") as f:\n",
        "    multi_hop_followers = pickle.load(f)\n",
        "\n",
        "with open(\"combined_hops.pkl\", \"rb\") as f:\n",
        "    combined_hops = pickle.load(f)\n",
        "\n",
        "\n",
        "with open(\"node_fairness_scores.pkl\", \"rb\") as f:\n",
        "    fairness_scores = pickle.load(f)\n",
        "\n",
        "graph_list=[]\n",
        "times=1\n",
        "while(times<3):\n",
        "    # random_choice = times\n",
        "    random_choice = np.random.choice([1, 2])\n",
        "    data_graph=func_graph_augmentation(random_choice)\n",
        "    graph_list.append(data_graph)\n",
        "    times=times+1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_loader,train_loader2=loader_func(graph_list[0],graph_list[1])\n",
        "for epoch in range(1, 501):\n",
        "\n",
        "    loss = train(train_loader,train_loader2,combined_hops,multi_hop_followers,fairness_scores)\n",
        "    print(f\"epoch {epoch}, loss: {loss}\")\n",
        "\n",
        "save_embeddings(model, train_loader, train_loader2, output_path_g1=\"embeddings_graph1.pkl\", output_path_g2=\"embeddings_graph2.pkl\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7sZb63tQcgd9",
        "outputId": "ce95c079-25c7-4d78-9bce-f1cd9ca4b368"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keys in pickle file: dict_keys(['node_ids', 'embeddings'])\n",
            " embeddings: [-0.0163359   0.59953594 -0.01570386  0.03813164 -0.07691559 -0.3168839\n",
            " -0.28525218 -0.5654796   0.09474057  0.34046745  0.2759269  -0.13153133\n",
            "  0.03908022 -0.26498938  0.15061544  0.02190842  0.07657653  0.4277137\n",
            "  0.06772633 -0.08555333  0.034463   -0.16574213 -0.11050139  0.0276746\n",
            " -0.21091281 -0.326487    0.08237393 -0.2976696   0.49293166  0.09948264\n",
            " -0.14257279 -0.04852556 -0.22392222 -0.41465214  0.40467486  0.26199389\n",
            "  0.44463763  0.3767833  -0.38445532  0.10642318 -0.04477096 -0.24914044\n",
            "  0.4613399  -0.2515414  -0.02934211  0.10974513 -0.01337276 -0.06883903\n",
            " -0.06811655  0.16130649]\n",
            " embeddings: [ 1.1786621  -3.901099    2.6943412   0.84948534  0.92163324 -0.6989604\n",
            " -1.6502064   4.254978   -1.098114    5.6994     -0.76367116 -0.5996207\n",
            "  2.1090372  -3.220058   -2.7920866   3.3065708   2.829766    1.9585996\n",
            " -0.46227753 -0.9545436  -3.2467198   4.6326737   1.1105989   1.2910306\n",
            "  6.099027   -3.131162   -1.1957335  -1.3161666   1.8610507   1.149824\n",
            "  1.5307457   3.5580704   2.6471043  -0.5517861   3.8964682  -1.0293652\n",
            "  2.0985727  -0.5102771  -0.5572327  -1.0802152   1.6803194   2.4001486\n",
            " -2.5249941  -3.4697611   1.599387    1.7241596  -1.9525086   0.5801303\n",
            "  1.4242027  -0.02603609]\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "\n",
        "# Load the pickle file\n",
        "with open(\"embeddings_graph1.pkl\", \"rb\") as f:\n",
        "    data = pickle.load(f)\n",
        "\n",
        "# Print keys\n",
        "print(\"Keys in pickle file:\", data.keys())\n",
        "for i in range(2):\n",
        "    print(f\" embeddings: {data['embeddings'][i]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keys in pickle file: dict_keys(['node_ids', 'embeddings'])\n",
            " embeddings: [-0.0163359   0.59953594 -0.01570386  0.03813164 -0.07691559 -0.3168839\n",
            " -0.28525218 -0.5654796   0.09474057  0.34046745  0.2759269  -0.13153133\n",
            "  0.03908022 -0.26498938  0.15061544  0.02190842  0.07657653  0.4277137\n",
            "  0.06772633 -0.08555333  0.034463   -0.16574213 -0.11050139  0.0276746\n",
            " -0.21091281 -0.326487    0.08237393 -0.2976696   0.49293166  0.09948264\n",
            " -0.14257279 -0.04852556 -0.22392222 -0.41465214  0.40467486  0.26199389\n",
            "  0.44463763  0.3767833  -0.38445532  0.10642318 -0.04477096 -0.24914044\n",
            "  0.4613399  -0.2515414  -0.02934211  0.10974513 -0.01337276 -0.06883903\n",
            " -0.06811655  0.16130649]\n",
            " embeddings: [ 1.1786621  -3.901099    2.6943412   0.84948534  0.92163324 -0.6989604\n",
            " -1.6502064   4.254978   -1.098114    5.6994     -0.76367116 -0.5996207\n",
            "  2.1090372  -3.220058   -2.7920866   3.3065708   2.829766    1.9585996\n",
            " -0.46227753 -0.9545436  -3.2467198   4.6326737   1.1105989   1.2910306\n",
            "  6.099027   -3.131162   -1.1957335  -1.3161666   1.8610507   1.149824\n",
            "  1.5307457   3.5580704   2.6471043  -0.5517861   3.8964682  -1.0293652\n",
            "  2.0985727  -0.5102771  -0.5572327  -1.0802152   1.6803194   2.4001486\n",
            " -2.5249941  -3.4697611   1.599387    1.7241596  -1.9525086   0.5801303\n",
            "  1.4242027  -0.02603609]\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "\n",
        "# Load the pickle file\n",
        "with open(\"embeddings_graph2.pkl\", \"rb\") as f:\n",
        "    data = pickle.load(f)\n",
        "\n",
        "# Print keys\n",
        "print(\"Keys in pickle file:\", data.keys())\n",
        "for i in range(2):\n",
        "    print(f\" embeddings: {data['embeddings'][i]}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "bubai",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
